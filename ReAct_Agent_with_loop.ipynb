{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPizl8hdRLk9Sct8WQy84WR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usernameforarihant/ReActAgent/blob/main/ReAct_Agent_with_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dKpv-Wf6m1L4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GROQ_API_KEY']=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcylPCjxndLs",
        "outputId": "a7669410-14b7-45f9-f095-f5d1a5a62842"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m112.6/121.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client=Groq()"
      ],
      "metadata": {
        "id": "IjwGsyLbpl-R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l66fvySXnjcC",
        "outputId": "d660a895-87f5-4868-aadb-470cd0982a1e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in today's technological landscape, revolutionizing various aspects of natural language processing (NLP) and human-computer interaction. Here are some reasons why fast language models are important:\n",
            "\n",
            "1. **Improved User Experience**: Fast language models enable applications to respond quickly to user queries, providing a seamless and engaging experience. This is particularly important for applications like chatbots, virtual assistants, and language translation software, where quick responses are essential for effective communication.\n",
            "2. **Increased Efficiency**: Fast language models can process large volumes of text data rapidly, making them ideal for tasks like data preprocessing, text analysis, and information retrieval. This increased efficiency can save time and resources, allowing businesses and organizations to focus on higher-level tasks.\n",
            "3. **Enhanced Accuracy**: Fast language models can analyze vast amounts of data, identify patterns, and make predictions more accurately than slower models. This can lead to better decision-making, improved sentiment analysis, and more effective language understanding.\n",
            "4. **Real-Time Applications**: Fast language models are essential for real-time applications like speech recognition, language translation, and text summarization. These models can process audio or text input in real-time, allowing for instantaneous responses and more effective communication.\n",
            "5. **Scalability**: Fast language models can handle large volumes of traffic and user queries, making them scalable for applications with high demand. This scalability is crucial for businesses and organizations that need to process large amounts of text data quickly and efficiently.\n",
            "6. **Edge Computing**: Fast language models can be deployed on edge devices like smartphones, smart home devices, and autonomous vehicles, enabling real-time processing and reducing latency. This is critical for applications that require immediate processing and response, such as voice assistants and autonomous driving.\n",
            "7. **Reduced Latency**: Fast language models can reduce latency in various applications, allowing for more responsive and interactive experiences. For example, language models can quickly generate text, translate languages, or summarize content, reducing the time it takes for users to receive the information they need.\n",
            "8. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage by providing faster, more accurate, and more efficient language processing capabilities. This can lead to improved customer satisfaction, increased market share, and reduced costs.\n",
            "9. **Scientific Research**: Fast language models can accelerate scientific research in areas like language acquisition, cognitive psychology, and neuroscience. By processing and analyzing large datasets quickly, researchers can gain insights into language patterns, cognitive processes, and brain function.\n",
            "10. **Future-Proofing**: Fast language models are essential for future-proofing applications, as they can adapt to evolving language patterns, new languages, and emerging technologies. By investing in fast language models, organizations can ensure they remain competitive and relevant in an increasingly language-driven world.\n",
            "\n",
            "To achieve fast language models, researchers and developers employ various techniques, such as:\n",
            "\n",
            "* Model pruning and compression\n",
            "* Quantization and knowledge distillation\n",
            "* Parallel processing and distributed computing\n",
            "* Attention mechanisms and efficient architectures\n",
            "* Pre-training and fine-tuning\n",
            "\n",
            "By leveraging these techniques, fast language models can be developed and deployed to drive innovation, improve user experiences, and unlock new possibilities in NLP and related fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjBwPkuXrIej",
        "outputId": "5b6179a0-bc82-476e-a12d-05d8ccd402d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-0bcf82e4-95f8-45ef-98c3-5b46afb3c1b5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\\n\\n1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\\n2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\\n3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\\n4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\\n5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\\n6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\\n7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\\n\\nSome of the key applications that benefit from fast language models include:\\n\\n1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\\n2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\\n3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\\n4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\\n5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\\n\\nTo achieve fast language models, researchers and developers use various techniques, such as:\\n\\n1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\\n2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\\n3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\\n4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\\n5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.', role='assistant', function_call=None, reasoning=None, tool_calls=None))], created=1741150406, model='llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ca0059abb', usage=CompletionUsage(completion_tokens=661, prompt_tokens=43, total_tokens=704, completion_time=2.403636364, prompt_time=0.004963764, queue_time=0.24776115899999998, total_time=2.408600128), x_groq={'id': 'req_01jnjadh8peeqskrja8t95vc5n'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8hREkszpjC7",
        "outputId": "64cc0d3e-f53e-4ec6-c9de-491e3823f931"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\\n\\n1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\\n2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\\n3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\\n4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\\n5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\\n6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\\n7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\\n\\nSome of the key applications that benefit from fast language models include:\\n\\n1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\\n2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\\n3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\\n4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\\n5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\\n\\nTo achieve fast language models, researchers and developers use various techniques, such as:\\n\\n1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\\n2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\\n3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\\n4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\\n5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.', role='assistant', function_call=None, reasoning=None, tool_calls=None))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po0HguRAquwk",
        "outputId": "904c7927-30c2-44d3-800f-bfc518067472"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\\n\\n1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\\n2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\\n3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\\n4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\\n5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\\n6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\\n7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\\n\\nSome of the key applications that benefit from fast language models include:\\n\\n1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\\n2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\\n3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\\n4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\\n5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\\n\\nTo achieve fast language models, researchers and developers use various techniques, such as:\\n\\n1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\\n2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\\n3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\\n4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\\n5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.', role='assistant', function_call=None, reasoning=None, tool_calls=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS8AwAJXqxfx",
        "outputId": "1dc3378c-7398-470c-d254-f408d127ff83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\n",
            "\n",
            "1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\n",
            "2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\n",
            "3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\n",
            "4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\n",
            "5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\n",
            "6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\n",
            "7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\n",
            "\n",
            "Some of the key applications that benefit from fast language models include:\n",
            "\n",
            "1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\n",
            "2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\n",
            "3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\n",
            "4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\n",
            "5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\n",
            "\n",
            "To achieve fast language models, researchers and developers use various techniques, such as:\n",
            "\n",
            "1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\n",
            "2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\n",
            "3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\n",
            "4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\n",
            "5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self,client,system):\n",
        "    self.client=client\n",
        "    self.system=system\n",
        "    self.messages=[]\n",
        "    if self.system is not None:\n",
        "      self.messages.append({\"role\":\"system\",\"content\":self.system})  #initial instructions for agent to follow\n",
        "  def __call__(self,message=\"\"):\n",
        "    if message:\n",
        "      self.messages.append({\"role\":\"user\",\"content\":message})\n",
        "    result=self.execute()\n",
        "    self.messages.append({\"role\":\"assistant\",\"content\":result})\n",
        "    return result\n",
        "  def execute(self):\n",
        "    completion=client.chat.completions.create(\n",
        "        messages=self.messages,\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "YcgmRR8urFF1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "get_planet_mass:\n",
        "e.g. get_planet_mass : Earth\n",
        "returns mass of the planet in kg\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: What is the mass of the Earth times 2?\n",
        "Thought: I need to find the mass of Earth\n",
        "Action: get_planet_mass: Earth\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 5.972e24\n",
        "\n",
        "Thought: I need to multilpy this by 2\n",
        "Action: calculate: 5.972e24 * 2\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 1,1944 x 10e25\n",
        "\n",
        "if you have the answer, output it as the Answer.\n",
        "\n",
        "Answer: The mass of the Earth times 2 is 1,1944 x 10e25 kg\n",
        "\n",
        "Now its your turn:\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "Hukh9RTbxjF_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tools\n",
        "def calculate(operation):\n",
        "  return eval(operation)\n",
        "\n",
        "def get_planet_mass(planet: str) -> float:\n",
        "    match planet.lower():\n",
        "        case \"earth\":\n",
        "            return 5.972e24\n",
        "        case \"mars\":\n",
        "            return 6.39e23\n",
        "        case \"jupiter\":\n",
        "            return 1.898e27\n",
        "        case \"saturn\":\n",
        "            return 5.683e26\n",
        "        case \"uranus\":\n",
        "            return 8.681e25\n",
        "        case \"neptune\":\n",
        "            return 1.024e26\n",
        "        case \"mercury\":\n",
        "            return 3.285e23\n",
        "        case \"venus\":\n",
        "            return 4.867e24\n",
        "        case _:\n",
        "            return 0.0  # Return None for unknown planets\n"
      ],
      "metadata": {
        "id": "HLqJ9mf41Nt_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "michael=Agent(client,system_prompt)"
      ],
      "metadata": {
        "id": "OcWVTGqb6tgr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=michael(\"What is the mass of the mars times 2?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLU4DGPo66H7",
        "outputId": "3a8fef3f-abd6-4f5d-c17e-a85450d62d60"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find the mass of Mars to calculate its mass times 2.\n",
            "Action: get_planet_mass: Mars\n",
            "PAUSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "michael.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tovsojqU7Be3",
        "outputId": "fd0925de-a5a0-4e7a-c565-706874b50472"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop you output an Answer\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\n\\nYour available actions are:\\n\\ncalculate:\\ne.g. calculate: 4 * 7 / 3\\nRuns a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\\nget_planet_mass:\\ne.g. get_planet_mass : Earth\\nreturns mass of the planet in kg\\n\\nExample session:\\n\\nQuestion: What is the mass of the Earth times 2?\\nThought: I need to find the mass of Earth\\nAction: get_planet_mass: Earth\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 5.972e24 \\n\\nThought: I need to multilpy this by 2\\nAction: calculate: 5.972e24 * 2\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 1,1944 x 10e25\\n\\nif you have the answer, output it as the Answer.\\n\\nAnswer: The mass of the Earth times 2 is 1,1944 x 10e25 kg\\n\\nNow its your turn:'},\n",
              " {'role': 'user', 'content': 'What is the mass of the mars times 2?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: I need to find the mass of Mars to calculate its mass times 2.\\nAction: get_planet_mass: Mars\\nPAUSE'}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=michael()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP7kwLiU7RmP",
        "outputId": "c53af871-d51c-4783-9f4b-cac5dcfe8aa4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation=get_planet_mass(\"mars\")\n",
        "print(observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C4m6SjJ7dQD",
        "outputId": "c1a304e8-3dd7-4c34-c660-156af00e0a62"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.39e+23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_prompt=f\"Observation: {observation}\"\n",
        "result=michael(next_prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se9cqnC07spR",
        "outputId": "b938235f-66d2-477a-9f55-3aa10d018e4e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: Now that I have the mass of Mars, I can multiply it by 2 to find the answer.\n",
            "Action: calculate: 6.39e+23 * 2\n",
            "PAUSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation=calculate(\"6.39e+23 * 2\")\n",
        "print(observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uokIBCHJ8HpA",
        "outputId": "63d4392e-fa5a-4e7b-f4d7-86f5047edbc6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.278e+24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_prompt=f\"Observation: {observation}\"\n",
        "result=michael(next_prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUKJIUvx8Sl6",
        "outputId": "772d21e6-6f0e-468b-9675-ccf51ea5e429"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I have now calculated the mass of Mars times 2, so I can provide the final answer.\n",
            "Answer: The mass of Mars times 2 is 1.278e+24 kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "michael.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCl0BKJf8s1J",
        "outputId": "f41c64de-0a3f-4008-aaaa-d85b85df89b1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop you output an Answer\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\n\\nYour available actions are:\\n\\ncalculate:\\ne.g. calculate: 4 * 7 / 3\\nRuns a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\\nget_planet_mass:\\ne.g. get_planet_mass : Earth\\nreturns mass of the planet in kg\\n\\nExample session:\\n\\nQuestion: What is the mass of the Earth times 2?\\nThought: I need to find the mass of Earth\\nAction: get_planet_mass: Earth\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 5.972e24 \\n\\nThought: I need to multilpy this by 2\\nAction: calculate: 5.972e24 * 2\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 1,1944 x 10e25\\n\\nif you have the answer, output it as the Answer.\\n\\nAnswer: The mass of the Earth times 2 is 1,1944 x 10e25 kg\\n\\nNow its your turn:'},\n",
              " {'role': 'user', 'content': 'What is the mass of the mars times 2?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: I need to find the mass of Mars to calculate its mass times 2.\\nAction: get_planet_mass: Mars\\nPAUSE'},\n",
              " {'role': 'assistant', 'content': ''},\n",
              " {'role': 'user', 'content': 'Observation: 6.39e+23'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: Now that I have the mass of Mars, I can multiply it by 2 to find the answer.\\nAction: calculate: 6.39e+23 * 2\\nPAUSE'},\n",
              " {'role': 'assistant', 'content': ''},\n",
              " {'role': 'user', 'content': 'Observation: 1.278e+24'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: I have now calculated the mass of Mars times 2, so I can provide the final answer.\\nAnswer: The mass of Mars times 2 is 1.278e+24 kg'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "qhoiSQAQ8wcz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loop(max_iterations=10, query: str = \"\"):\n",
        "\n",
        "    agent = Agent(client=client, system=system_prompt)\n",
        "\n",
        "    tools = [\"calculate\", \"get_planet_mass\"]\n",
        "\n",
        "    next_prompt = query\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < max_iterations:\n",
        "        i += 1\n",
        "        result = agent(next_prompt)\n",
        "        print(result)\n",
        "\n",
        "        if \"PAUSE\" in result and \"Action\" in result:\n",
        "            action = re.findall(r\"Action: ([a-z_]+): (.+)\", result, re.IGNORECASE)\n",
        "            chosen_tool = action[0][0]\n",
        "            arg = action[0][1]\n",
        "\n",
        "            if chosen_tool in tools:\n",
        "                result_tool = eval(f\"{chosen_tool}('{arg}')\")\n",
        "                next_prompt = f\"Observation: {result_tool}\"\n",
        "\n",
        "            else:\n",
        "                next_prompt = \"Observation: Tool not found\"\n",
        "\n",
        "            print(next_prompt)\n",
        "            continue\n",
        "\n",
        "        if \"Answer\" in result:\n",
        "            break"
      ],
      "metadata": {
        "id": "rFxhdsCPMo3-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loop(query=\"What is the mass of Earth plus the mass of Saturn and all of that times 2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8DlVF1APV8P",
        "outputId": "302ccf62-2e82-461c-ea8b-0eb8eaac663f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: To find the mass of Earth plus the mass of Saturn and all of that times 2, I first need to find the masses of Earth and Saturn. I will start by finding the mass of Earth.\n",
            "\n",
            "Action: get_planet_mass: Earth\n",
            "PAUSE\n",
            "Observation: 5.972e+24\n",
            "Thought: Now that I have the mass of Earth, I need to find the mass of Saturn. Once I have both masses, I can add them together and then multiply the result by 2.\n",
            "\n",
            "Action: get_planet_mass: Saturn\n",
            "PAUSE\n",
            "Observation: 5.683e+26\n",
            "Thought: I now have the masses of Earth and Saturn. I will add these two masses together and then multiply the result by 2 to get the final answer.\n",
            "\n",
            "Action: calculate: (5.972e24 + 5.683e26) * 2\n",
            "PAUSE\n",
            "Observation: 1.148544e+27\n",
            "Thought: I have calculated the mass of Earth plus the mass of Saturn and multiplied the result by 2. I now have the final answer.\n",
            "\n",
            "Answer: The mass of Earth plus the mass of Saturn and all of that times 2 is 1.148544e+27 kg.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JJmLKXkPLdw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SSPf5BkyPbvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}