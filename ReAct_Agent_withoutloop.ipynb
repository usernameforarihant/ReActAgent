{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLTNwIsv0d5ADeh+ntI5cM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dKpv-Wf6m1L4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GROQ_API_KEY']=\"enter api key\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcylPCjxndLs",
        "outputId": "96d7f328-75bb-4a91-9d42-48fd512a20b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client=Groq()"
      ],
      "metadata": {
        "id": "IjwGsyLbpl-R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l66fvySXnjcC",
        "outputId": "db647e6a-ff3e-4dd8-f332-ae9ac8f35c37"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\n",
            "\n",
            "1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\n",
            "2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\n",
            "3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\n",
            "4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\n",
            "5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\n",
            "6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\n",
            "7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\n",
            "\n",
            "Some of the key applications that benefit from fast language models include:\n",
            "\n",
            "1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\n",
            "2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\n",
            "3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\n",
            "4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\n",
            "5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\n",
            "\n",
            "To achieve fast language models, researchers and developers use various techniques, such as:\n",
            "\n",
            "1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\n",
            "2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\n",
            "3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\n",
            "4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\n",
            "5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjBwPkuXrIej",
        "outputId": "5b6179a0-bc82-476e-a12d-05d8ccd402d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-0bcf82e4-95f8-45ef-98c3-5b46afb3c1b5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\\n\\n1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\\n2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\\n3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\\n4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\\n5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\\n6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\\n7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\\n\\nSome of the key applications that benefit from fast language models include:\\n\\n1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\\n2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\\n3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\\n4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\\n5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\\n\\nTo achieve fast language models, researchers and developers use various techniques, such as:\\n\\n1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\\n2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\\n3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\\n4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\\n5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.', role='assistant', function_call=None, reasoning=None, tool_calls=None))], created=1741150406, model='llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ca0059abb', usage=CompletionUsage(completion_tokens=661, prompt_tokens=43, total_tokens=704, completion_time=2.403636364, prompt_time=0.004963764, queue_time=0.24776115899999998, total_time=2.408600128), x_groq={'id': 'req_01jnjadh8peeqskrja8t95vc5n'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8hREkszpjC7",
        "outputId": "64cc0d3e-f53e-4ec6-c9de-491e3823f931"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\\n\\n1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\\n2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\\n3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\\n4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\\n5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\\n6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\\n7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\\n\\nSome of the key applications that benefit from fast language models include:\\n\\n1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\\n2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\\n3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\\n4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\\n5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\\n\\nTo achieve fast language models, researchers and developers use various techniques, such as:\\n\\n1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\\n2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\\n3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\\n4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\\n5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.', role='assistant', function_call=None, reasoning=None, tool_calls=None))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po0HguRAquwk",
        "outputId": "904c7927-30c2-44d3-800f-bfc518067472"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\\n\\n1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\\n2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\\n3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\\n4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\\n5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\\n6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\\n7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\\n\\nSome of the key applications that benefit from fast language models include:\\n\\n1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\\n2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\\n3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\\n4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\\n5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\\n\\nTo achieve fast language models, researchers and developers use various techniques, such as:\\n\\n1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\\n2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\\n3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\\n4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\\n5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.', role='assistant', function_call=None, reasoning=None, tool_calls=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS8AwAJXqxfx",
        "outputId": "1dc3378c-7398-470c-d254-f408d127ff83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in natural language processing (NLP) as they enable efficient and effective processing of human language. The importance of fast language models can be understood from several perspectives:\n",
            "\n",
            "1. **Real-time Applications**: Fast language models are essential for real-time applications, such as language translation, sentiment analysis, and chatbots. These models need to process and respond to user input quickly, usually within milliseconds. Slow models can lead to delayed responses, which can be frustrating for users and affect their overall experience.\n",
            "2. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications. This is particularly important for applications that require processing large amounts of text data, such as text classification, named entity recognition, and language modeling.\n",
            "3. **Low-Latency**: Fast language models can reduce latency, which is critical for applications that require fast and responsive interactions, such as voice assistants, virtual customer service agents, and online language learning platforms.\n",
            "4. **Energy Efficiency**: Fast language models can be more energy-efficient, as they require less computational power and memory to process language tasks. This is important for applications that run on mobile devices, edge devices, or in data centers where energy consumption is a concern.\n",
            "5. **Improved User Experience**: Fast language models can improve the user experience by providing quick and accurate responses, which can lead to higher user engagement, satisfaction, and retention.\n",
            "6. **Competitive Advantage**: Fast language models can provide a competitive advantage for businesses that rely on NLP, such as language translation services, chatbot platforms, and text analytics companies.\n",
            "7. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to test and validate their ideas more quickly and efficiently.\n",
            "\n",
            "Some of the key applications that benefit from fast language models include:\n",
            "\n",
            "1. **Virtual assistants**: Virtual assistants like Siri, Alexa, and Google Assistant rely on fast language models to understand and respond to user voice commands.\n",
            "2. **Language translation**: Fast language models are used in language translation services, such as Google Translate, to provide quick and accurate translations.\n",
            "3. **Chatbots**: Chatbots use fast language models to understand and respond to user input, providing customer support and answering frequently asked questions.\n",
            "4. **Text classification**: Fast language models are used in text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\n",
            "5. **Language learning**: Fast language models are used in language learning platforms to provide personalized feedback and correction to language learners.\n",
            "\n",
            "To achieve fast language models, researchers and developers use various techniques, such as:\n",
            "\n",
            "1. **Model pruning**: Removing unnecessary weights and connections in the model to reduce computational complexity.\n",
            "2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller model.\n",
            "3. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\n",
            "4. **Parallelization**: Using parallel processing techniques, such as data parallelism or model parallelism, to speed up model training and inference.\n",
            "5. **Hardware acceleration**: Using specialized hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), to accelerate model training and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self,client,system):\n",
        "    self.client=client\n",
        "    self.system=system\n",
        "    self.messages=[]\n",
        "    if self.system is not None:\n",
        "      self.messages.append({\"role\":\"system\",\"content\":self.system})  #initial instructions for agent to follow\n",
        "  def __call__(self,message=\"\"):\n",
        "    if message:\n",
        "      self.messages.append({\"role\":\"user\",\"content\":message})\n",
        "    result=self.execute()\n",
        "    self.messages.append({\"role\":\"assistant\",\"content\":result})\n",
        "    return result\n",
        "  def execute(self):\n",
        "    completion=client.chat.completions.create(\n",
        "        messages=self.messages,\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "YcgmRR8urFF1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "get_planet_mass:\n",
        "e.g. get_planet_mass : Earth\n",
        "returns mass of the planet in kg\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: What is the mass of the Earth times 2?\n",
        "Thought: I need to find the mass of Earth\n",
        "Action: get_planet_mass: Earth\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 5.972e24\n",
        "\n",
        "Thought: I need to multilpy this by 2\n",
        "Action: calculate: 5.972e24 * 2\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 1,1944 x 10e25\n",
        "\n",
        "if you have the answer, output it as the Answer.\n",
        "\n",
        "Answer: The mass of the Earth times 2 is 1,1944 x 10e25 kg\n",
        "\n",
        "Now its your turn:\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "Hukh9RTbxjF_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tools\n",
        "def calculate(operation):\n",
        "  return eval(operation)\n",
        "\n",
        "def get_planet_mass(planet: str) -> float:\n",
        "    match planet.lower():\n",
        "        case \"earth\":\n",
        "            return 5.972e24\n",
        "        case \"mars\":\n",
        "            return 6.39e23\n",
        "        case \"jupiter\":\n",
        "            return 1.898e27\n",
        "        case \"saturn\":\n",
        "            return 5.683e26\n",
        "        case \"uranus\":\n",
        "            return 8.681e25\n",
        "        case \"neptune\":\n",
        "            return 1.024e26\n",
        "        case \"mercury\":\n",
        "            return 3.285e23\n",
        "        case \"venus\":\n",
        "            return 4.867e24\n",
        "        case _:\n",
        "            return 0.0  # Return None for unknown planets\n"
      ],
      "metadata": {
        "id": "HLqJ9mf41Nt_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "michael=Agent(client,system_prompt)"
      ],
      "metadata": {
        "id": "OcWVTGqb6tgr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=michael(\"What is the mass of the mars times 2?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLU4DGPo66H7",
        "outputId": "3a8fef3f-abd6-4f5d-c17e-a85450d62d60"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find the mass of Mars to calculate its mass times 2.\n",
            "Action: get_planet_mass: Mars\n",
            "PAUSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "michael.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tovsojqU7Be3",
        "outputId": "fd0925de-a5a0-4e7a-c565-706874b50472"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop you output an Answer\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\n\\nYour available actions are:\\n\\ncalculate:\\ne.g. calculate: 4 * 7 / 3\\nRuns a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\\nget_planet_mass:\\ne.g. get_planet_mass : Earth\\nreturns mass of the planet in kg\\n\\nExample session:\\n\\nQuestion: What is the mass of the Earth times 2?\\nThought: I need to find the mass of Earth\\nAction: get_planet_mass: Earth\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 5.972e24 \\n\\nThought: I need to multilpy this by 2\\nAction: calculate: 5.972e24 * 2\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 1,1944 x 10e25\\n\\nif you have the answer, output it as the Answer.\\n\\nAnswer: The mass of the Earth times 2 is 1,1944 x 10e25 kg\\n\\nNow its your turn:'},\n",
              " {'role': 'user', 'content': 'What is the mass of the mars times 2?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: I need to find the mass of Mars to calculate its mass times 2.\\nAction: get_planet_mass: Mars\\nPAUSE'}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=michael()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP7kwLiU7RmP",
        "outputId": "c53af871-d51c-4783-9f4b-cac5dcfe8aa4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation=get_planet_mass(\"mars\")\n",
        "print(observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C4m6SjJ7dQD",
        "outputId": "c1a304e8-3dd7-4c34-c660-156af00e0a62"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.39e+23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_prompt=f\"Observation: {observation}\"\n",
        "result=michael(next_prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se9cqnC07spR",
        "outputId": "b938235f-66d2-477a-9f55-3aa10d018e4e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: Now that I have the mass of Mars, I can multiply it by 2 to find the answer.\n",
            "Action: calculate: 6.39e+23 * 2\n",
            "PAUSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation=calculate(\"6.39e+23 * 2\")\n",
        "print(observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uokIBCHJ8HpA",
        "outputId": "63d4392e-fa5a-4e7b-f4d7-86f5047edbc6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.278e+24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_prompt=f\"Observation: {observation}\"\n",
        "result=michael(next_prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUKJIUvx8Sl6",
        "outputId": "772d21e6-6f0e-468b-9675-ccf51ea5e429"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I have now calculated the mass of Mars times 2, so I can provide the final answer.\n",
            "Answer: The mass of Mars times 2 is 1.278e+24 kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "michael.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCl0BKJf8s1J",
        "outputId": "f41c64de-0a3f-4008-aaaa-d85b85df89b1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop you output an Answer\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\n\\nYour available actions are:\\n\\ncalculate:\\ne.g. calculate: 4 * 7 / 3\\nRuns a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\\nget_planet_mass:\\ne.g. get_planet_mass : Earth\\nreturns mass of the planet in kg\\n\\nExample session:\\n\\nQuestion: What is the mass of the Earth times 2?\\nThought: I need to find the mass of Earth\\nAction: get_planet_mass: Earth\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 5.972e24 \\n\\nThought: I need to multilpy this by 2\\nAction: calculate: 5.972e24 * 2\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: 1,1944 x 10e25\\n\\nif you have the answer, output it as the Answer.\\n\\nAnswer: The mass of the Earth times 2 is 1,1944 x 10e25 kg\\n\\nNow its your turn:'},\n",
              " {'role': 'user', 'content': 'What is the mass of the mars times 2?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: I need to find the mass of Mars to calculate its mass times 2.\\nAction: get_planet_mass: Mars\\nPAUSE'},\n",
              " {'role': 'assistant', 'content': ''},\n",
              " {'role': 'user', 'content': 'Observation: 6.39e+23'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: Now that I have the mass of Mars, I can multiply it by 2 to find the answer.\\nAction: calculate: 6.39e+23 * 2\\nPAUSE'},\n",
              " {'role': 'assistant', 'content': ''},\n",
              " {'role': 'user', 'content': 'Observation: 1.278e+24'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: I have now calculated the mass of Mars times 2, so I can provide the final answer.\\nAnswer: The mass of Mars times 2 is 1.278e+24 kg'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhoiSQAQ8wcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}